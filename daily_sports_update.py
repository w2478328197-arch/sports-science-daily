import feedparser
import datetime
import os
import ssl
import re
import urllib.request
import urllib.parse
import json
import argparse
import time
import xml.etree.ElementTree as ET
from datetime import timezone

def load_env_file(filepath):
    """Load .env file manually without external dependencies"""
    env_vars = {}
    if os.path.exists(filepath):
        with open(filepath, 'r') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key_val = line.strip().split('=', 1)
                    if len(key_val) == 2:
                        env_vars[key_val[0]] = key_val[1]
    return env_vars

# Load .env variables
env_vars = load_env_file(os.path.join(os.path.dirname(__file__), '.env'))

# Notion ÈÖçÁΩÆ (‰ºòÂÖà‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèÔºåÂÖ∂Ê¨°‰ΩøÁî® .env Êñá‰ª∂)
NOTION_TOKEN = os.environ.get("NOTION_TOKEN", env_vars.get("NOTION_TOKEN", ""))
NOTION_PAGE_ID = os.environ.get("NOTION_PAGE_ID", env_vars.get("NOTION_PAGE_ID", ""))

# ËÑöÊú¨ÈÖçÁΩÆ
ITEMS_PER_FEED = 20         # Â¢ûÂä†Âà∞ 20 ‰ª•Èò≤Ê≠¢ÂÉè Huawei Central ËøôÊ†∑È´òÈ¢ëÊõ¥Êñ∞ÁöÑÊ∫ê‰∏¢Â§±Êï∞ÊçÆ
TRANSLATION_LIMIT = 2000   # ÊëòË¶ÅÁøªËØëÂ≠óÊï∞ÈôêÂà∂ (Â¢ûÂä†‰ª•ÂÆπÁ∫≥ÂÆåÊï¥ÊëòË¶Å)
RETMAX_PUBMED = 50         # PubMed ÊØèÊ¨°Ê£ÄÁ¥¢Êï∞Èáè (Increased from 15 to 50 for broader coverage)

# ÈÖçÁΩÆ SSL (Ëß£ÂÜ≥Êüê‰∫õÁéØÂ¢É‰∏ãÁöÑËØÅ‰π¶ÈóÆÈ¢ò)
if hasattr(ssl, '_create_unverified_context'):
    ssl._create_default_https_context = ssl._create_unverified_context

# --- 1. Á≤æÈÄâÁßëÁ†î/‰∏ìÂÆ∂Ê∫ê (Whitelist Only) ---
# ‰∏•Ê†ºÁ≠õÈÄâÔºåÊåâÁÖßÁî®Êà∑ÁöÑ‰∏âÂ§ßÊùøÂùóÈáçÁªÑ

# ÊùøÂùó 1: ÊàëÂñúÊ¨¢ÁöÑÂçö‰∏ªÁöÑÂä®Âêë (ÂêàÂπ∂‰∫ÜÂéüÊúâÁöÑÁßëÁ†îÂçöÂÆ¢Âíå‰∏ìÂÆ∂È¢ëÈÅì)
# ÊùøÂùó 1: ÊàëÂñúÊ¨¢ÁöÑÂçö‰∏ªÁöÑÂä®Âêë (ÂêàÂπ∂‰∫ÜÂéüÊúâÁöÑÁßëÁ†îÂçöÂÆ¢Âíå‰∏ìÂÆ∂È¢ëÈÅì)
# ÊùøÂùó 1: ÊàëÂñúÊ¨¢ÁöÑÂçö‰∏ªÁöÑÂä®Âêë (ÂêàÂπ∂‰∫ÜÂéüÊúâÁöÑÁßëÁ†îÂçöÂÆ¢Âíå‰∏ìÂÆ∂È¢ëÈÅì)
BLOGGER_FEEDS = [
    ("Stronger by Science (Nuckols)", "https://www.strongerbyscience.com/feed/"),
    ("Mysportscience (Jeukendrup)", "https://www.mysportscience.com/blog-feed.xml"),
    ("Peter Attia (Longevity)", "https://peterattiadrive.libsyn.com/rss"),
    ("Andrew Huberman (Podcast)", "https://feeds.megaphone.fm/hubermanlab"),
    ("Science for Sport", "https://www.scienceforsport.com/feed/"), 
    ("YLMSportScience", "https://ylmsportscience.com/feed/"),
    ("Bryan Johnson (Blueprint)", "https://www.youtube.com/feeds/videos.xml?channel_id=UCnRVL1-HJnXWB_Xi2dAoTcg"),
    ("Jeff Nippard (Science Explained)", "https://www.youtube.com/feeds/videos.xml?channel_id=UCjTp-nBKswYLumqmVeBPwYw"),
    ("Renaissance Periodization (Dr. Mike)", "https://www.youtube.com/feeds/videos.xml?channel_id=UCfQgsKhHjSyRLOp9mnffqVg"),
    ("Andy Galpin (Human Performance)", "https://www.youtube.com/feeds/videos.xml?channel_id=UCe3R2e3zYxWwIhMKV36Qhkw"),
    ("JB Morin (Biomechanics)", "https://jb-morin.net/feed/"),
]

# ÊùøÂùó 2: Êô∫ËÉΩÂèØÁ©øÊà¥Âä®Âêë (Fitbit/Whoop/Garmin/Apple/Oura)
INDUSTRY_FEEDS = [
    ("DC Rainmaker (Wearable Tech)", "https://www.dcrainmaker.com/feed"),
    ("Google Research (Health & Bioscience)", "https://research.google/blog/rss/"),
    ("Whoop Podcast (Recovery Science)", "https://feeds.buzzsprout.com/230442.rss"),
    ("Oura Engineering (Tech Blog)", "https://ouraring.wpengine.com/category/meet-oura/feed/"), # Ensuring we get technical posts
    ("Fitbit (Google Blog)", "https://blog.google/products/fitbit/rss/"),
    ("Garmin Blog", "https://www.garmin.com/en-US/blog/feed/"),
    ("Polar Blog", "https://www.polar.com/blog/feed/"),
    ("Oura Ring Blog", "https://ouraring.com/blog/feed/"),
    ("Apple Newsroom (Health)", "https://www.apple.com/newsroom/rss-feed.rss"),
]

RSS_FEEDS = {
    "bloggers": BLOGGER_FEEDS,
    "industry": INDUSTRY_FEEDS
}

# PubMed È°∂ÂàäÂàóË°® (Êâ©Â±ïËá≥ 22 ‰∏™ÂÖ®ÁêÉÈ°∂Â∞ñÊúüÂàä)
PUBMED_JOURNALS = [
    # ËøêÂä®ÂåªÂ≠¶/‰∏¥Â∫äÁ±ª
    "British Journal of Sports Medicine",
    "The American Journal of Sports Medicine",
    "Sports Medicine",
    "Scandinavian Journal of Medicine & Science in Sports",
    "Knee Surgery, Sports Traumatology, Arthroscopy",
    "Journal of Orthopaedic & Sports Physical Therapy",
    "Sports Health",
    "Clinical Journal of Sport Medicine",
    
    # ÁîüÁêÜÂ≠¶/ÁîüÁâ©ÂåñÂ≠¶Á±ª
    "Journal of Applied Physiology",
    "European Journal of Applied Physiology",
    "Journal of Strength and Conditioning Research",
    "Medicine and Science in Sports and Exercise",
    
    # Ë°®Áé∞/Â∫îÁî®ÁßëÂ≠¶Á±ª
    "International Journal of Sports Physiology and Performance",
    "Journal of Sports Sciences",
    "Journal of Sport and Health Science",
    "Sports Medicine-Open",
    
    # Ëê•ÂÖª/Ë°å‰∏∫Á±ª
    "International Journal of Sport Nutrition and Exercise Metabolism",
    "Journal of the International Society of Sports Nutrition",
    "International Journal of Behavioral Nutrition and Physical Activity",
    "Nutrients"
]

# --- ËæÖÂä©ÂáΩÊï∞ ---

def translate_to_chinese(text, retries=3):
    """Google Translate Web API with simple retry logic"""
    if not text: return ""
    
    # È¢ÑÂ§ÑÁêÜÔºö‰øùÊåÅÊÆµËêΩÁªìÊûÑÔºå‰ΩÜÂéªÈô§Â§ö‰ΩôÁ©∫ÁôΩ
    text = re.sub(r'\s+', ' ', text).strip()
    if len(text) > TRANSLATION_LIMIT: text = text[:TRANSLATION_LIMIT] + "..."
    
    url = "https://translate.googleapis.com/translate_a/single"
    params = {"client": "gtx", "sl": "auto", "tl": "zh-CN", "dt": "t", "q": text}
    full_url = f"{url}?{urllib.parse.urlencode(params)}"
    
    for attempt in range(retries):
        try:
            req = urllib.request.Request(full_url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as response:
                data = json.loads(response.read().decode('utf-8'))
                # ÊãºÊé•ÁøªËØëÁªìÊûú
                return "".join([x[0] for x in data[0] if x[0]])
        except Exception as e:
            if attempt == retries - 1:
                print(f"    ‚ö†Ô∏è Translation failed after {retries} attempts: {e}")
                return text # Fallback to original
            time.sleep(1)
            
    return text

def is_recent(entry_date_struct, days):
    """Check if date is within lookback period"""
    if not entry_date_struct: return False
    now = datetime.datetime.now(timezone.utc)
    try:
        # entry_date_struct usually is time.struct_time
        # convert to aware datetime
        pub_date = datetime.datetime(*entry_date_struct[:6], tzinfo=timezone.utc)
        delta = now - pub_date
        return delta.days < days
    except:
        return False # Fail safe

# ÂéÜÂè≤ËÆ∞ÂΩïÊñá‰ª∂
HISTORY_FILE = os.path.join(os.path.dirname(__file__), 'processed_history.json')

def load_history():
    """Load processed links from history file"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except Exception as e:
            print(f"    ‚ö†Ô∏è Warning: Could not load history: {e}")
            return set()
    return set()

def save_history(history_set):
    """Save processed links to history file"""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(list(history_set), f, indent=2)
    except Exception as e:
        print(f"    ‚ö†Ô∏è Warning: Could not save history: {e}")

# --- Ê†∏ÂøÉÊäìÂèñ ---

def fetch_rss_feeds(days, history_set, disable_history=False):
    print("üì° Ê≠£Âú®ÊäìÂèñÁ≤æÈÄâ‰∏ìÂÆ∂Ê∫ê (RSS)...")
    content = {}
    new_links = set()
    
    for category, feeds in RSS_FEEDS.items():
        print(f"  üìÇ {category}")
        items = []
        for name, url in feeds:
            try:
                # Robust Fetching with User-Agent
                req = urllib.request.Request(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/rss+xml, application/xml, text/xml, */*'
                })
                try:
                    with urllib.request.urlopen(req, timeout=15) as response:
                        xml_content = response.read()
                        feed = feedparser.parse(xml_content)
                except Exception as net_err:
                    print(f"    ‚ö†Ô∏è Network/Auth Error for {name}: {net_err} - Trying direct parse...")
                    feed = feedparser.parse(url) # Fallback

                if feed.bozo and feed.bozo_exception:
                    # Ignore common encoding errors if entries exist
                    if len(feed.entries) == 0:
                        print(f"    ‚ö†Ô∏è Feed Parse Error (BOZO) for {name}: {feed.bozo_exception}")
                        continue
                
                count = 0
                for entry in feed.entries:
                    if count >= ITEMS_PER_FEED: break
                    
                    try:
                        # Date Check
                        date_struct = getattr(entry, 'published_parsed', None) or getattr(entry, 'updated_parsed', None)
                        if not is_recent(date_struct, days): continue
                        
                        # Safe Link Parsing (Fix for Huberman/others)
                        link = getattr(entry, 'link', '')
                        if not link and hasattr(entry, 'links'):
                            for l in entry.links:
                                if l.get('rel') == 'alternate':
                                    link = l.get('href')
                                    break
                        
                        if not link:
                            # print(f"    ‚ö†Ô∏è Skipping entry with no link: {getattr(entry, 'title', 'No Title')[:30]}")
                            continue
                        
                        # Deduplication Check
                        if not disable_history and link in history_set:
                            # print(f"    ‚ÑπÔ∏è Skipping (Already processed): {entry.title[:30]}...")
                            continue
                        
                        title = getattr(entry, 'title', 'No Title')
                        
                        summary = ""
                        if hasattr(entry, 'summary'): summary = entry.summary
                        elif hasattr(entry, 'description'): summary = entry.description
                        elif hasattr(entry, 'media_description'): summary = entry.media_description
                        
                        # ‰ºòÂÖàÂ§ÑÁêÜ YouTube ËßÜÈ¢ëÊèèËø∞
                        if hasattr(entry, 'media_group'):
                            media_desc = entry.media_group.get('media_description', '')
                            if media_desc:
                                summary = media_desc
                        
                        # Clean HTML for translation
                        clean_summary = re.sub(r'<[^>]+>', ' ', summary)
                        clean_summary = " ".join(clean_summary.split())[:500] 
                        
                        print(f"    Processing: {title[:30]}...")
                        title_zh = translate_to_chinese(title)
                        summary_zh = translate_to_chinese(clean_summary)
                        
                        items.append({
                            'title': title_zh,
                            'orig_title': title,
                            'link': link,
                            'summary': summary_zh,
                            'orig_summary': summary, # Store original for filtering
                            'source': name
                        })
                        new_links.add(link)
                        count += 1
                        
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è Error processing entry from {name}: {e}")
                        continue
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error fetching {name}: {e}")
        
        if items: content[category] = items
        
    return content, new_links

def fetch_pubmed_abstracts(days, history_set, disable_history=False):
    print(f"üìö Ê≠£Âú®Ê∑±Â∫¶ÊäìÂèñ PubMed È°∂ÂàäËÆ∫Êñá (Last {days} days)...")
    
    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    # --- PubMed Blocklist (Exclude Surgery/Animals) ---
    PUBMED_BLOCKLIST = [
        "rat", "rats", "mouse", "mice", "murine", "animal", "porcine", "cadaver", "cadaveric", "vitro",
        "surgery", "surgical", "reconstruction", "arthroscopy", "arthroplasty", "graft", "implant", "prosthesis",
        "cancer", "chemotherapy", "tumor", "metastasis", "oncology"
    ]
    pubmed_block_pattern = re.compile(
        r'\b(' + '|'.join(map(re.escape, PUBMED_BLOCKLIST)) + r')\b',
        re.IGNORECASE
    )
    
    # 1. Search
    journal_query = " OR ".join([f'"{j}"[Journal]' for j in PUBMED_JOURNALS])
    # Add date filter
    query = f'({journal_query}) AND ("last {days} days"[dp])'
    
    search_url = f"{base_url}/esearch.fcgi?db=pubmed&term={urllib.parse.quote(query)}&retmode=json&retmax={RETMAX_PUBMED}"
    
    new_links = set()
    
    try:
        req = urllib.request.Request(search_url)
        with urllib.request.urlopen(req) as r:
            data = json.loads(r.read().decode('utf-8'))
            id_list = data.get('esearchresult', {}).get('idlist', [])
            
        if not id_list:
            print("    ‚ÑπÔ∏è Ê≤°ÊúâÂèëÁé∞Êñ∞ËÆ∫Êñá")
            return [], new_links
        
        # Filter IDs based on history (link construction: https://pubmed.ncbi.nlm.nih.gov/{pmid}/)
        unique_id_list = []
        for pmid in id_list:
            link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            if not disable_history and link in history_set:
                continue
            unique_id_list.append(pmid)
            
        if not unique_id_list:
             print("    ‚ÑπÔ∏è ÊâÄÊúâÊñ∞ËÆ∫ÊñáÂùáÂ∑≤Â§ÑÁêÜËøá„ÄÇ")
             return [], new_links

        print(f"    üîç ÂèëÁé∞ {len(unique_id_list)} ÁØáÊñ∞ËÆ∫Êñá (ÊÄªÊï∞ {len(id_list)})ÔºåÊ≠£Âú®Ëß£ÊûêÊëòË¶Å...")
        
        # 2. Fetch Details (XML)
        ids = ",".join(unique_id_list)
        fetch_url = f"{base_url}/efetch.fcgi?db=pubmed&id={ids}&retmode=xml"
        
        req = urllib.request.Request(fetch_url)
        papers = []
        with urllib.request.urlopen(req) as r:
            tree = ET.fromstring(r.read())
            
            for article in tree.findall(".//PubmedArticle"):
                try:
                    # Basic Info
                    title = article.findtext(".//ArticleTitle") or "No Title"
                    journal = article.findtext(".//Journal/Title") or "Unknown Journal"
                    pmid = article.findtext(".//PMID")
                    if not pmid: continue
                    link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                    
                    # Abstract Parsing (Structured)
                    abstract_parts = []
                    abstract_texts = article.findall(".//AbstractText")
                    
                    if abstract_texts:
                        for elem in abstract_texts:
                            label = elem.get('Label') # e.g., BACKGROUND, METHODS, RESULTS
                            text = elem.text
                            if not text: continue
                            
                            if label:
                                abstract_parts.append(f"**{label.title()}**: {text}")
                            else:
                                abstract_parts.append(text)
                        full_abstract = "\n\n".join(abstract_parts)
                    else:
                        full_abstract = "No abstract available."
                    
                    
                    # --- Blocklist Check ---
                    text_to_check = (title + " " + full_abstract).lower()
                    if pubmed_block_pattern.search(text_to_check):
                        print(f"    üö´ Blocked (Topic): {title[:50]}...")
                        # Add to history to prevent re-fetching
                        new_links.add(link) 
                        continue
                        
                    # Translation
                    print(f"    üìÑ Translating: {title[:30]}...")
                    title_zh = translate_to_chinese(title)
                    # Translate abstract paragraph by paragraph to maintain structure
                    abstract_zh_parts = []
                    for part in abstract_parts:
                        if part.startswith("**"):
                            # Handle labeled parts: "**Methods**: content"
                            split_idx = part.find(": ")
                            if split_idx != -1:
                                label_part = part[:split_idx+2]
                                content_part = part[split_idx+2:]
                                abstract_zh_parts.append(f"{label_part}{translate_to_chinese(content_part)}")
                            else:
                                abstract_zh_parts.append(translate_to_chinese(part))
                        else:
                            abstract_zh_parts.append(translate_to_chinese(part))
                    
                    abstract_zh = "\n\n".join(abstract_zh_parts) if abstract_zh_parts else translate_to_chinese(full_abstract)
                    
                    papers.append({
                        'title': title_zh,
                        'orig_title': title,
                        'source': journal,
                        'link': link,
                        'summary': abstract_zh
                    })
                    new_links.add(link)
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Error parsing paper: {e}")
                    continue
                    
        return papers, new_links
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è PubMed Search Error: {e}")
        return [], new_links

# --- Markdown & Notion ---

def generate_markdown(rss_data, pubmed_data):
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    md_lines = []
    
    md_lines.append(f"# üß¨ ËøêÂä®ÁßëÂ≠¶Êó•Êä• (Research Grade) - {today}")
    md_lines.append(f"> ËøôÈáåÂè™ÂÖ≥Ê≥®ÊúÄÁ°¨Ê†∏ÁöÑÁßëÂ≠¶ÔºöPubMed È°∂ÂàäËß£Êûê‰∏é‰∏ìÂÆ∂Ê∑±Â∫¶ÂçöÂÆ¢„ÄÇ")
    md_lines.append("")
    
    # 1. ÊàëÂñúÊ¨¢ÁöÑÂçö‰∏ªÁöÑÂä®Âêë
    md_lines.append("## 1. ÊàëÂñúÊ¨¢ÁöÑÂçö‰∏ªÁöÑÂä®Âêë")
    md_lines.append("")
    
    blogger_items = rss_data.get("bloggers", [])
    if blogger_items:
        for item in blogger_items:
            md_lines.append(f"### {item['title']}")
            md_lines.append(f"**Êù•Ê∫ê:** {item['source']} | [ÂéüÊñáÈìæÊé•]({item['link']})")
            md_lines.append("")
            md_lines.append(f"> {item['summary']}")
            md_lines.append("")
            md_lines.append("---")
            md_lines.append("")
    else:
        md_lines.append("*(‰ªäÊó•ÊöÇÊó†Êõ¥Êñ∞)*")
        md_lines.append("")

    # 2. Ë°å‰∏öÂä®Âêë
    md_lines.append("## 2. Ë°å‰∏öÁßëÁ†î‰∏éÊäÄÊúØÂ∑•Á®ã (Industry Research & Engineering)")
    md_lines.append("")
    
    industry_items = rss_data.get("industry", [])
    
    # ÂÖ≥ÈîÆËØçËøáÊª§ (‰ªÖ‰øùÁïôËøêÂä®ÂÅ•Â∫∑Áõ∏ÂÖ≥)
    # ÂÖ≥ÈîÆËØçËøáÊª§ (‰ªÖ‰øùÁïôËøêÂä®ÂÅ•Â∫∑Áõ∏ÂÖ≥)
    POSITIVE_KEYWORDS = [
        "health", "fitness", "sport", "run", "running", "swim", "cycle", "cycling", "ride", 
        "train", "training", "exercise", "workout", "sleep", "recovery", "rest",
        "heart", "hrv", "pulse", "oxygen", "blood", "glucose", "monitor", "vital", "stress",
        "watch", "smartwatch", "band", "ring", "wearable", "tracker", 
        "coach", "athlete", "marathon", "triathlon",
        "muscle", "cardio", "aerobic", "anaerobic", "vo2", "calorie", "step", "activity",
        "motion", "movement", "wellness", "physio", "biometric", "body",
        "metabolic", "metabolism", "altitude", "acclimation", "heat", "cold"
    ]
    
    # ÂøÖÈ°ªÂåÖÂê´ÁöÑÁßëÁ†î/Á°¨Ê†∏ÂÖ≥ÈîÆËØç (User Request: Focus on Research & Technical Blogs)
    RESEARCH_KEYWORDS = [
        "research", "study", "science", "scientist", "clinical", "publication", "paper", "journal",
        "algorithm", "validation", "whitepaper", "engineering", "technology", "tech", "lab", 
        "measure", "accuracy", "biomarker", "sensor", "data", "analysis", "insight", "review",
        "deep dive", "explained", "how it works", "behind the scenes", "validity", "reliability",
        "testing", "beta", "update", "feature", "metric", "physiology",
        "ai", "artificial intelligence", "machine learning", "neural network", "deep learning", "model",
        # ÂèØÁ©øÊà¥ËÆæÂ§áÁ†îÁ©∂‰∏ìÁî®ËØç
        "strain", "readiness", "load", "trend", "score", "stage", "zone", "baseline",
        "track", "detect", "predict", "alert", "notification", "insight", "optimize",
        "sleep architecture", "circadian", "rem", "deep sleep", "light sleep",
        "resting heart rate", "respiratory rate", "spo2", "temperature", "skin temp"
    ]
    
    # Âèó‰ø°‰ªªÁöÑÂéÇÂïÜÂÆòÊñπÂçöÂÆ¢ (ÂØπËøô‰∫õÊ∫êÊîæÂÆΩËøáÊª§Ë¶ÅÊ±Ç)
    TRUSTED_SOURCES = [
        "garmin", "oura", "polar", "fitbit", "whoop", "dc rainmaker",
        "google research", "apple"
    ]
    
    NEGATIVE_KEYWORDS = [
        "shareholder", "dividend", "financial results", "quarterly", "revenue", "profit",
        "phone", "phones", "smartphone", "smartphones", "mobile", "mobiles", 
        "camera", "cameras", "lens", "lenses", "laptop", "laptops", "notebook", "notebooks", 
        "tv", "tvs", "television", "televisions",
        "car", "cars", "automotive", "auto", "vehicle", "vehicles",
        "headphone", "headphones", "earbud", "earbuds", 
        "movie", "movies", "cinema", "film", "films", "motion picture",
        "video game", "gaming", "console", "consoles", 
        "music", "album", "song", "songs", "artist", "artists", "award", "awards",
        "investment", "stock", "stocks", "discount", "bundle", "clearance", "sale", "sales",
        "aviation", "cockpit", "flight", "pilot",  # Garmin Aviation
        "marathon training", "training plan", "join our", "program" # Generic Training Plans
    ]
    
    # Pre-compile regex for negative keywords (Word Boundary check)
    negative_pattern = re.compile(
        r'\b(' + '|'.join(map(re.escape, NEGATIVE_KEYWORDS)) + r')\b',
        re.IGNORECASE
    )
    
    filtered_industry_items = []
    
    for item in industry_items:
        # CRITICAL FIX: Use original English text for filtering
        text_to_check = (item.get('orig_title', '') + " " + item.get('orig_summary', '')).lower()
        if not text_to_check.strip():
             text_to_check = (item['title'] + " " + item['summary']).lower() # Fallback
        
        # Ëé∑ÂèñÊ∫êÂêçÁß∞Áî®‰∫éÁôΩÂêçÂçïÊ£ÄÊü•
        source_name = item.get('source', '').lower()
        is_trusted_source = any(ts in source_name for ts in TRUSTED_SOURCES)
        
        # 1. ÂøÖÈ°ªÂåÖÂê´Ëá≥Â∞ë‰∏Ä‰∏™ÊôÆÈÄöÂÖ≥ÈîÆËØç (Topic)
        has_positive = any(pk in text_to_check for pk in POSITIVE_KEYWORDS)
        
        # 2. ÂøÖÈ°ªÂåÖÂê´Ëá≥Â∞ë‰∏Ä‰∏™ÁßëÁ†î/Á°¨Ê†∏ÂÖ≥ÈîÆËØç (Depth)
        has_research = any(rk in text_to_check for rk in RESEARCH_KEYWORDS)

        # 3. ‰∏çËÉΩÂåÖÂê´‰ªª‰ΩïË¥üÈù¢ÂÖ≥ÈîÆËØç
        has_negative = bool(negative_pattern.search(text_to_check))
        
        # Ë±ÅÂÖçÈÄªËæëÔºöÂØπÂèó‰ø°‰ªªÊ∫êÊîæÂÆΩÊ∑±Â∫¶Ë¶ÅÊ±Ç
        STRONG_KEYWORDS = ["validation", "accuracy", "algorithm", "whitepaper"]
        has_strong = any(sk in text_to_check for sk in STRONG_KEYWORDS)
        
        if has_negative and not has_strong:
            is_relevant = False
        else:
            # Ê†∏ÂøÉÈÄªËæëÂçáÁ∫ß:
            # - Âèó‰ø°‰ªªÊ∫ê: Âè™Ë¶ÅÊúâ Topic ÂÖ≥ÈîÆËØçÂç≥ÂèØ (ÊîæÂÆΩ Depth Ë¶ÅÊ±Ç)
            # - ÂÖ∂‰ªñÊ∫ê: ÂøÖÈ°ªÂêåÊó∂Êúâ Topic + Depth
            if is_trusted_source:
                is_relevant = has_positive  # Âèó‰ø°‰ªªÊ∫êÊîæÂÆΩ: Âè™ÈúÄ Topic
            else:
                is_relevant = has_positive and (has_research or has_strong)
            
        if is_relevant:
            filtered_industry_items.append(item)
        else:
            print(f"    ‚ùå Rejected: {item.get('orig_title', item['title'])[:30]}... (Pos:{has_positive}, Res:{has_research}, Str:{has_strong}, Trusted:{is_trusted_source})")
    
    if filtered_industry_items:
        for item in filtered_industry_items:
            # Check if title/summary needs translation (it hasn't been translated yet in the main flow)
            title = item['title']
            summary = item['summary']
            
            md_lines.append(f"### {translate_to_chinese(title)}")
            md_lines.append(f"**Êù•Ê∫ê:** {item['source']} | [ÂéüÊñáÈìæÊé•]({item['link']})")
            md_lines.append("")
            md_lines.append(f"> {translate_to_chinese(summary)}")
            md_lines.append("")
            md_lines.append("---")
            md_lines.append("")
    else:
        md_lines.append("*(‰ªäÊó•ÊöÇÊó†Êõ¥Êñ∞)*")
        md_lines.append("")

    # 3. ÁßëÁ†îËøõÂ±ï
    md_lines.append("## 3. ÁßëÁ†îËøõÂ±ï (PubMed È°∂Âàä)")
    md_lines.append("")
    
    if pubmed_data:
        for paper in pubmed_data:
            md_lines.append(f"### {paper['title']}")
            md_lines.append(f"**ÊúüÂàä:** *{paper['source']}* | [ÂéüÊñáÈìæÊé•]({paper['link']})")
            md_lines.append("")
            md_lines.append(paper['summary'])
            md_lines.append("")
            md_lines.append("---")
            md_lines.append("")
    else:
        md_lines.append("*(‰ªäÊó•ÊöÇÊó†Êñ∞ÂèëË°®ËÆ∫Êñá)*")
        md_lines.append("")
            
    return "\n".join(md_lines)

def parse_markdown_to_notion_blocks(markdown_text):
    """
    Improved Markdown parser for Notion blocks.
    Handles headers, bold, links, and paragraphs.
    """
    blocks = []
    lines = markdown_text.split('\n')
    
    current_paragraph = []
    
    for line in lines:
        line = line.rstrip() # keep leading spaces if needed, but remove trailing
        
        # If empty line, flush paragraph
        if not line:
            if current_paragraph:
                text_content = "\n".join(current_paragraph)
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {"rich_text": parse_rich_text(text_content)}
                })
                current_paragraph = []
            continue
            
        # Check for Headers or Divider
        if line.startswith("# ") or line.startswith("## ") or line.startswith("### ") or line.startswith("---"):
            # Flush previous paragraph first
            if current_paragraph:
                text_content = "\n".join(current_paragraph)
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {"rich_text": parse_rich_text(text_content)}
                })
                current_paragraph = []
                
            if line.startswith("# "):
                pass # Main title skipped
            elif line.startswith("## "):
                blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": parse_rich_text(line[3:])}})
            elif line.startswith("### "):
                blocks.append({"object": "block", "type": "heading_3", "heading_3": {"rich_text": parse_rich_text(line[4:])}})
            elif line.startswith("---"):
                blocks.append({"object": "block", "type": "divider", "divider": {}})
        
        elif line.startswith("> "):
            # Blockquote
             if current_paragraph:
                text_content = "\n".join(current_paragraph)
                blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": parse_rich_text(text_content)}})
                current_paragraph = []
             blocks.append({"object": "block", "type": "quote", "quote": {"rich_text": parse_rich_text(line[2:])}})
        
        else:
            # Accumulate paragraph lines
            current_paragraph.append(line)
            
    # Flush remaining
    if current_paragraph:
        text_content = "\n".join(current_paragraph)
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": parse_rich_text(text_content)}
        })

    return blocks

def parse_rich_text(text):
    """
    Parses **bold** and [link](url).
    """
    parts = []
    # Regex for [text](url)
    link_pattern = re.compile(r'\[([^\]]+)\]\((http[^\)]+)\)')
    # Regex for **bold**
    bold_pattern = re.compile(r'\*\*([^\*]+)\*\*')
    
    # We will split by links first, then process bold inside non-link parts
    last_idx = 0
    for match in link_pattern.finditer(text):
        pre_text = text[last_idx:match.start()]
        if pre_text:
            parts.extend(process_bold(pre_text, bold_pattern))
            
        link_text = match.group(1)
        link_url = match.group(2)
        parts.append({
            "type": "text",
            "text": {"content": link_text, "link": {"url": link_url}}
        })
        last_idx = match.end()
        
    if last_idx < len(text):
        parts.extend(process_bold(text[last_idx:], bold_pattern))
        
    return parts

def process_bold(text, pattern):
    results = []
    last_idx = 0
    for match in pattern.finditer(text):
        pre = text[last_idx:match.start()]
        if pre: results.append({"type": "text", "text": {"content": pre}})
        
        bold_content = match.group(1)
        results.append({
            "type": "text",
            "text": {"content": bold_content},
            "annotations": {"bold": True}
        })
        last_idx = match.end()
        
    if last_idx < len(text):
        results.append({"type": "text", "text": {"content": text[last_idx:]}})
    return results

def sync_to_notion(blocks, token, page_id):
    if not token or not page_id:
        print("‚ÑπÔ∏è Notion Token Êàñ Page ID Êú™ËÆæÁΩÆÔºåË∑≥Ëøá‰∏ä‰º†„ÄÇ")
        print("   (ËØ∑ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè NOTION_TOKEN Âíå NOTION_PAGE_ID Âú® setup_cron.sh ‰∏≠)")
        return

    print("üîÑ ÂêåÊ≠•Âà∞ Notion...")
    
    def notion_request(endpoint, data, method="POST"):
        url = f"https://api.notion.com/v1/{endpoint}"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }
        try:
            req = urllib.request.Request(url, data=json.dumps(data).encode('utf-8'), headers=headers, method=method)
            with urllib.request.urlopen(req) as response:
                return json.loads(response.read().decode('utf-8'))
        except urllib.error.HTTPError as e:
            print(f"   ‚ùå Notion API Error: {e.code} - {e.read().decode('utf-8')}")
            return None

    # 1. Create Page
    today_title = f"üß¨ ËøêÂä®ÁßëÂ≠¶Êó•Êä• (Research Grade) - {datetime.datetime.now().strftime('%Y-%m-%d')}"
    resp = notion_request("pages", {
        "parent": {"page_id": page_id},
        "properties": {"title": {"title": [{"text": {"content": today_title}}]}}
    })
    
    if not resp: return
    new_page_id = resp['id']
    
    # 2. Upload in batches
    batch_size = 90
    for i in range(0, len(blocks), batch_size):
        batch = blocks[i:i+batch_size]
        notion_request(f"blocks/{new_page_id}/children", {"children": batch}, method="PATCH")
        print(f"   Â∑≤‰∏ä‰º† {len(batch)} ‰∏™Âå∫Âùó...")
        
    print(f"‚ú® ÂêåÊ≠•ÂÆåÊàêÔºÅÈ°µÈù¢: https://notion.so/{new_page_id.replace('-', '')}")

# --- ‰∏ªÁ®ãÂ∫è ---

def main():
    parser = argparse.ArgumentParser(description="Sports Science Daily Crawler")
    parser.add_argument("--days", type=int, default=7, help="Lookback days for new content (default: 7)")
    parser.add_argument("--no-history", action="store_true", help="Disable history checking (fetching all items)")
    args = parser.parse_args()
    
    print(f"üöÄ ÂêØÂä®ËøêÂä®ÁßëÂ≠¶Áà¨Ëô´ V3.1 (Deduplication Enabled) - Lookback: {args.days} days")
    
    history_set = load_history()
    print(f"üìö Â∑≤Âä†ËΩΩ {len(history_set)} Êù°ÂéÜÂè≤ËÆ∞ÂΩï")
    
    rss_data, rss_new_links = fetch_rss_feeds(args.days, history_set, args.no_history)
    pubmed_data, pubmed_new_links = fetch_pubmed_abstracts(args.days, history_set, args.no_history)
    
    # Ê£ÄÊü•ÊòØÂê¶ÊúâÊñ∞ÂÜÖÂÆπ
    total_new = len(rss_new_links) + len(pubmed_new_links)
    if total_new == 0:
        print("üéâ Ê≤°ÊúâÂèëÁé∞Êñ∞ÂÜÖÂÆπ (ÊâÄÊúâÈ°πÁõÆÂùáÂ∑≤Âú®‰πãÂâçÁöÑËøêË°å‰∏≠Â§ÑÁêÜËøá)„ÄÇ")
        return

    md_content = generate_markdown(rss_data, pubmed_data)
    
    # Save local file
    filename = f"{datetime.datetime.now().strftime('%Y-%m-%d')}_ËøêÂä®ÁßëÂ≠¶Êó•Êä•.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"üíæ Êú¨Âú∞Êñá‰ª∂Â∑≤‰øùÂ≠ò: {filename}")
    
    # Notion Sync
    blocks = parse_markdown_to_notion_blocks(md_content)
    sync_to_notion(blocks, NOTION_TOKEN, NOTION_PAGE_ID)
    
    # Update History
    if not args.no_history:
        history_set.update(rss_new_links)
        history_set.update(pubmed_new_links)
        save_history(history_set)
        print(f"üîñ Êõ¥Êñ∞ÂéÜÂè≤ËÆ∞ÂΩï: Êñ∞Â¢û {total_new} Êù°È°πÁõÆÔºåÊÄªËÆ° {len(history_set)} Êù°„ÄÇ")

if __name__ == "__main__":
    main()
